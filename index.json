[
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Enhance Data Visibility with Cribl Search and Amazon Managed Grafana In today’s digital environment, organizations face challenges in managing the growing volume of operational data across their infrastructure—logs, metrics, and traces from applications and systems.\nThis information holds the key to deeper insights and performance improvement. However, to use it effectively, organizations need a scalable and customizable observability pipeline that can collect, process, and route data to the right destinations.\nCribl \u0026amp; Amazon Managed Grafana Cribl, an AWS APN partner, provides centralized data management and configurable routing solutions for large volumes of operational and security data.\nAmazon Managed Grafana is used to visualize data processed by Cribl, turning it into actionable dashboards with real-time insights.\nThe integration of Cribl Search with Amazon Managed Grafana enables more powerful monitoring and analysis, helping organizations make faster, more reliable, and large-scale data-driven decisions.\nKey Use Cases 1. Cloud infrastructure monitoring Cribl Search can query data from sources such as Amazon S3, Cribl Lake, Amazon Security Lake, or native AWS services through APIs, without requiring prior indexing. Results can then be sent via Cribl Stream to SIEM systems for analysis.\nGrafana is then used to build real-time dashboards showing resource usage, costs, and performance across AWS regions and related services.\n2. Application performance management Create application-specific dashboards: latency, error rates, user experience, with drill-down capabilities to analyze transactions in detail.\n3. Security operations Display security events via dedicated dashboards, improving incident response time and investigation. Cribl supports continuous monitoring of security events, compliance reporting, and enhanced threat detection.\nPrerequisites AWS account with administrative access Amazon S3 bucket (existing or newly created) and VPC Flow Logs with write access to the bucket Proper IAM configuration for users enabling flow logs Cribl Cloud account Implementation Steps 1. Configure API authentication Use API tokens to secure communication between Cribl and Amazon Managed Grafana. Access “API Credentials” in the Cribl dashboard to obtain Client ID, secret, etc.\n2. Install and configure the plugin Go to Amazon Managed Grafana \u0026gt; Plugins \u0026gt; “Add new connection” \u0026gt; search for “Cribl” plugin \u0026gt; add connection using Cribl details.\n3. Create your first visualization In Grafana, use the Query tab to run a sample query retrieving VPC Flow Logs from the last 15 minutes, grouped by log status every minute.\n4. Explore deeper with Table Search in Amazon Managed Grafana Switch from time-series chart to table view to see detailed log entries, identify anomalies, and trace requests. For example, query 1,000 records from a dataset to support investigations.\nCleanup and Cost Considerations Delete VPC Flow Logs and temporary S3 buckets when no longer needed. Remove unused Cribl configurations to avoid unnecessary costs. Conclusion The integration of Cribl and Amazon Managed Grafana creates a customizable observability pipeline that centralizes data management, enhances security and compliance, and provides visualized data to support actionable decisions. This solution is valuable for enterprises seeking large-scale observability that is future-ready.\n"
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Enabling customers to deliver production-ready AI agents at scale AI agents are ushering in a major technological revolution, similar to the birth of the Internet. AWS is committed to building the best platform for developing and deploying secure, reliable, and scalable AI agents. Real-world applications have demonstrated great potential in healthcare (AstraZeneca), finance (Yahoo Finance), and agriculture (Syngenta). To expand on this success, AWS is focused on providing tools and platforms that make it easy for organizations to move AI agents from prototype to production, with a focus on security, reliability, and adaptability over time.\nGuiding Principles for Agentic AI AWS structures its approach around four core principles:\nEmbrace Agility\nBuild systems that are flexible, modular, and able to evolve as models, capabilities, and requirements change. :contentReference[oaicite:3]{index=3}\nEvolve Fundamentals\nTailor core infrastructure elements for the agentic era, including:\nSecurity \u0026amp; Trust (session isolation, memory isolation) :contentReference[oaicite:4]{index=4} Reliability \u0026amp; Scalability (checkpointing, recovery, scaling to many concurrent sessions) :contentReference[oaicite:5]{index=5} Identity (fine-grained permissions, integrating with identity providers) :contentReference[oaicite:6]{index=6} Observability (real-time monitoring, telemetry) :contentReference[oaicite:7]{index=7} Data access \u0026amp; use, including integrating proprietary data securely :contentReference[oaicite:8]{index=8} Seamless Integration with existing tools, clouds, agents, APIs etc. :contentReference[oaicite:9]{index=9} Deliver Superior Outcomes with Model Choice \u0026amp; Data\nAllow customers to choose what model(s) fit their needs (reasoning, speed, cost, etc.). :contentReference[oaicite:10]{index=10} Provide tools for model customization: fine-tuning, alignment, pre/post training, etc. :contentReference[oaicite:11]{index=11} Introduce solutions like Amazon Nova customization (including PEFT, full fine-tuning, etc.) and Nova Act for browser-based agents. :contentReference[oaicite:12]{index=12} Introduce Amazon S3 Vectors for storing vector embeddings more cheaply while maintaining performance, enabling richer context and memory for agents. :contentReference[oaicite:13]{index=13} Deploy Solutions that Transform Experiences\nProvide pre-built agentic solutions and tools, so organizations don’t need to build everything from scratch. :contentReference[oaicite:14]{index=14} Use AWS Marketplace to access curated agents/tools. :contentReference[oaicite:15]{index=15} Examples: Kiro (IDE for spec-driven development), AWS Transform (agents for code analysis, modernization tasks), Amazon Connect with AI for customer interactions. :contentReference[oaicite:16]{index=16} New Capabilities \u0026amp; Announcements AgentCore: a set of services for deploying and operating agents at enterprise scale — with runtime, identity, observability, memory, browser, code interpreter tools, etc. :contentReference[oaicite:17]{index=17} Nova Customization in SageMaker AI: expanded capabilities for pre-training, fine-tuning, alignment using various techniques to customize models. :contentReference[oaicite:18]{index=18} Nova Act SDK: browser-automation-focused agents, currently in preview. :contentReference[oaicite:19]{index=19} S3 Vectors: native vector support in S3, reducing vector storage cost by ~90% while keeping low latency query. :contentReference[oaicite:20]{index=20} Marketplace Agents \u0026amp; Tools: curated catalog of agents \u0026amp; tools from AWS Partners, simpler procurement and deployment. :contentReference[oaicite:21]{index=21} Pre-built Solutions: tools like Kiro and AWS Transform to accelerate moving from concept to production. :contentReference[oaicite:22]{index=22} Recommendations \u0026amp; Path Forward Start now: pick a specific business problem to prototype. Don’t wait for perfect conditions. :contentReference[oaicite:23]{index=23} Collect real-world feedback and iterate quickly. :contentReference[oaicite:24]{index=24} AWS is investing heavily (e.g. Generative AI Innovation Center) to support customers. :contentReference[oaicite:25]{index=25} Conclusion AWS is laying down a comprehensive foundation to enable organizations to build, customize, secure, and scale AI agents. With new tools and services (AgentCore, Nova, S3 Vectors, etc.), the focus is on moving beyond experiments into production systems, ensuring trustworthiness, adaptability, and business value.\n"
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Goal-Oriented Growth: Driving AWS Marketplace Success with COSS Cloud marketplaces have evolved into a powerful channel for technology companies to reach customers. According to Canalys projections, by 2028, enterprise software sales via cloud marketplaces will reach $85 billion USD, with many organizations now using AWS Marketplace as their preferred procurement channel for software, data, and professional services. :contentReference[oaicite:0]{index=0}\nTo help partners succeed, AWS introduces the COSS (Characteristics of Successful Sellers) framework, comprising six pillars that embody strategic, business, and technical best practices. AWS research shows that partners who adopt the COSS framework tend to grow 31% faster in Marketplace sales compared to peers. :contentReference[oaicite:1]{index=1}\nAWS Partner Commitment to Marketplace Revenue A foundational pillar of the COSS framework is ensuring leadership commitment to AWS Marketplace revenue. Success starts at the top—executives must set explicit goals, signal priority, and align the entire organization around Marketplace growth. :contentReference[oaicite:2]{index=2}\nWhy this focus matters:\nOrganizational Alignment: Revenue goals help unify product, sales, marketing, and operations toward the same objectives. :contentReference[oaicite:3]{index=3} Resource Prioritization: When Marketplace revenue is an explicit target, teams are more willing to allocate budgets, headcount, and technical resources to support it. :contentReference[oaicite:4]{index=4} Accountability \u0026amp; Measurement: Clear goals transform Marketplace from a side channel into a core, measurable business initiative with defined ownership and metrics. :contentReference[oaicite:5]{index=5} Setting Goals Across Teams To turn top-level ambition into results, organizations should cascade output goals and input goals across functions:\nFunction / Team Example Goals \u0026amp; Actions Sales Integrate Marketplace targets into compensation plans; set quotas for deal size, win rates, and segment-specific deals via Marketplace. :contentReference[oaicite:6]{index=6} Operations / Enablement Define enablement targets for legal, finance, sales ops; automate private offer workflows; integrate Marketplace metrics into sales ops dashboards. :contentReference[oaicite:7]{index=7} Product / Marketing Optimize listings, drive traffic \u0026amp; conversion; build collateral, co-marketing plans; embrace product-led growth mechanisms. :contentReference[oaicite:8]{index=8} Training / Onboarding Embed Marketplace education into new hire programs; schedule recurring training sessions for sellers, operations, and cross-functional stakeholders. :contentReference[oaicite:9]{index=9} By aligning goals at all levels, you transform AWS Marketplace from an afterthought into a key growth engine.\nImplementation Steps \u0026amp; Best Practices Set a Defined Revenue Goal\nStart with a concrete target: a proportion of your total revenue you aim to transact through AWS Marketplace, or a growth rate target for your Marketplace business. :contentReference[oaicite:10]{index=10}\nDrive Organizational Alignment\nHold regular cross-functional working sessions; communicate the strategic importance of Marketplace; ensure all teams understand how they contribute. :contentReference[oaicite:11]{index=11}\nAlign Incentives\nEnsure your sales compensation plan includes explicit incentives tied to Marketplace sales. This helps motivate behavioral change. :contentReference[oaicite:12]{index=12}\nMonitor, Adjust, Celebrate\nUse metrics and dashboards to track performance, identify gaps, pivot where needed, and celebrate wins across teams to maintain momentum. :contentReference[oaicite:13]{index=13}\nPath Forward \u0026amp; Recommendations Begin now—choose a product or use case to pilot in the Marketplace, rather than waiting for perfect timing. :contentReference[oaicite:14]{index=14} Use early feedback and data to iterate quickly. Demonstrating real progress helps your leadership team maintain commitment and enables stronger co-sell relationships with AWS. Embed the COSS framework deeply in your operations, not as an afterthought but as a core business approach. Conclusion The AWS Marketplace opportunity is vast—but success requires intentionality. By committing to defined goals, aligning incentives, and applying rigorous practices across teams, partners can transform the Marketplace into a scalable, high-impact growth engine. With the COSS framework as your guide, the journey toward Marketplace excellence becomes clearer, measurable, and purposeful.\n"
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/3-blogstranslated/3.4-blog4/",
	"title": "Blog 4",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/3-blogstranslated/3.5-blog5/",
	"title": "Blog 5",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/3-blogstranslated/3.6-blog6/",
	"title": "Blog 6",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report “Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders” General Introduction Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders was a special livestream event organized by Amazon Web Services (AWS), connecting the builder community in Ho Chi Minh City with the main conference held in Hanoi.\nThis edition aimed to provide a platform for developers, architects, and technical leaders in the South of Vietnam to engage with the latest cloud, AI, and modernization trends, while participating in discussions and technical sessions broadcast directly from Hanoi.\nEvent Objectives Connect the AWS builder community in Ho Chi Minh City with experts and keynotes from the main event in Hanoi. Share the latest innovations in Cloud Computing, Generative AI, and Application Modernization. Provide insights into best practices for designing scalable, secure, and resilient systems on AWS. Encourage knowledge exchange and networking among cloud professionals, startups, and organizations in the region. Inspire builders to adopt modern architectures and apply AI responsibly in real-world projects. List of Speakers Conor McNamara – Managing Director, AWS ASEAN Alois Reitbauer – Vice President \u0026amp; Chief Technical Evangelist, Dynatrace Erica Liu – Senior GTM Specialist, App Modernization, AWS Fabrianne Effendi – Associate Specialist SA, Serverless, AWS Jignesh Shah – Director, Open Source Databases, AWS Tran Nhat Quang – Solutions Architect Lead, AWS Vietnam Nguyen Thi Thuy Linh – Senior Partner Manager, AWS Vietnam (Speakers presented live in Hanoi and were streamed to the Ho Chi Minh City Connect Edition.)\nKey Highlights 1. Opening Keynote (Livestream from Hanoi) Overview of Vietnam’s cloud adoption journey and AWS’s contribution to national digital transformation. Launch of AWS regional initiatives supporting startups, SMEs, and education in Vietnam. Introduction to Generative AI on AWS – empowering innovation through safe, scalable AI solutions. 2. Builders Session (Local Discussion) Exploration of Event-Driven Architecture (EDA) and Domain-Driven Design (DDD) in building cloud-native systems. Hands-on examples of microservices modernization, focusing on scalability, decoupling, and resilience. Demonstration of Amazon Q Developer, an AI-driven assistant across the entire Software Development Lifecycle (SDLC). 3. Networking \u0026amp; Community Interaction Interactive discussions between builders, developers, and AWS community members. Shared experiences on career development in cloud computing and DevOps. Insights into AWS training paths, certifications, and the AWS Community Builder program. Key Learnings Understood the principles of modern architectures and how to design for event-driven communication. Learned how Generative AI and Amazon Q Developer enhance developer productivity and innovation. Gained awareness of modernization tools and frameworks for cloud migration. Recognized the value of collaboration and community learning in professional growth. Application in Work Applied Domain-Driven Design and Event-Driven Architecture concepts to current backend projects. Explored AWS Lambda and serverless workflows for lightweight event-driven applications. Adopted Amazon Q Developer as a productivity tool for planning, refactoring, and testing. Encouraged teammates to adopt a business-first and domain-oriented design mindset. Experience at the Event Experienced a seamless livestream connection to the main AWS Vietnam Cloud Day in Hanoi. Gained technical and business insights from industry experts. Participated in interactive discussions with the local AWS builder community. The event provided both in-depth technical learning and networking opportunities, strengthening my cloud expertise. Event Photos Conclusion:\nThe event successfully connected the Southern AWS builder community with global experts and key thought leaders. It provided valuable insights into cloud modernization, AI-driven development, and emphasized the power of collaboration and knowledge sharing across regions.\n"
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report: DATA SCIENCE ON AWS WORKSHOP Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah – Director, Open Source Databases Erica Liu – Sr. GTM Specialist, AppMod Fabrianne Effendi – Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles → Lost revenue/missed opportunities Inefficient operations → Reduced productivity, higher costs Non-compliance with security regulations → Security breaches, loss of reputation Transitioning to modern application architecture – Microservices Migrating to a modular system — each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events → arrange timeline → identify actors → define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 → ECS → Fargate → Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing — follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the “GenAI-powered App-DB Modernization” workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos "
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Đặng Đình Bắc\nPhone Number: 0352437611\nEmail: bacddse180351@fpt.edu.vn\nUniversity: FPTU HCM\nMajor: Software Engineer\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 09/09/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Introduction to Automated Remediation Automated Remediation is the use of software and logical rules to detect and fix infrastructure issues without human intervention. They are critical components for building \u0026ldquo;self-healing\u0026rdquo; systems that minimize service downtime. Computing resources running in production may encounter unexpected issues (such as memory overflow, application crashes). Through the combination of Amazon CloudWatch (monitoring) and AWS Systems Manager (execution), the system can automatically respond to these issues immediately instead of waiting for engineers to handle them manually. Workshop Overview In this workshop, you will deploy a completely automated monitoring and issue remediation process.\nWeb-Server-Test is an EC2 Instance resource that acts as a test application server. A CloudWatch Agent has been installed and configured within this instance to collect memory (RAM) metrics – a metric not monitored by default by AWS. You will use the stress tool to simulate a memory overflow attack, causing the server to become unresponsive. EventBridge \u0026amp; Systems Manager simulate an automated operations team. A Rule is configured to listen for alerts from CloudWatch. When memory exceeds the safe threshold, the system will automatically trigger a restart command (Reboot) on the instance through Systems Manager Automation. This mechanism simulates rapid service recovery in real-world environments. In this workshop, we perform the Reboot action to see results immediately, but for production workloads, you can configure more complex procedures such as dumping RAM for error analysis before restarting or replacing instances with Auto Scaling. "
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic Cloud computing and AWS service Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Study Module 01-01: What is Cloud Computing? - Study Module 01-02: What Makes AWS Different? - Study Module 01-03: How to Start Your Cloud Journey. 09/08/2025 09/08/2025 https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=1/ 3 - Study Module 01-04: AWS Global Infrastructure - Study Module 01-05: AWS services management tools - Study Module 01-06: Optimizing Costs on AWS and Working with AWS Support - Understand AWS Regions, Availability Zones, core services (EC2, S3, RDS). 09/09/2025 09/09/2025 https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=1/ 4 - Study Module 01-07: Additional practice and research - Learn about AWS Free Tier, AWS Console, and AWS CLI. 09/10/2025 09/10/2025 https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=1/ 5 - Do Lab 01-01: Create an AWS account. - Practice: + Create AWS account + Configure billing settings, check Free Tier status. 09/11/2025 09/11/2025 https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=1/ 6 - Do Lab 01-02: Setup with Virtual MFA Device - Practice: + Setup with Virtual MFA Device - Learn what MFA is and why it’s important. 09/12/2025 09/12/2025 https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=1/ 7 - Do Lab 01-03: Create admin group and admin user. - Practice: + Create admin group and admin user - Explain the difference between the root account and IAM users 09/13/2025 09/13/2025 https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=1/ Sun - Write Worklog Week 1 - Schedules tasks for week 2 - Start brainstorming ideas for the Project Proposal 09/14/2025 09/14/2025 https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=1/ Week 1 Achievements: Get to know the members of FCJ Created a Free tier AWS account "
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Learn about VPC in AWS Tasks to be carried out this week: Day Task Start Date Completion Date Reference 2 - Learn about the concept of VPC - Module 02-01 - AWS Virtual Private Cloud 15/09/2025 15/09/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=1/ 3 - Learn about VPC Security and Multi-VPC features - Module 02-02 - VPC Security and Multi-VPC features 16/09/2025 16/09/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=1/ 4 - Learn about VPN, DirectConnect, LoadBalancer, and ExtraResources for connecting on-premise and cloud environments - Module 02-03 - VPN - DirectConnect - LoadBalancer - ExtraResources 17/09/2025 17/09/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=1/ 5 - Attend Vietnam Cloud Day 2025 - Start lab to create VPC 18/09/2025 18/09/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=1/ 6 - Perform lab to launch EC2 instance 19/09/2025 19/09/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=1/ 7 - Practice and repeat lab exercises 20/09/2025 20/09/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=1/ Sun - Write weekly work journal for Week 2 - Plan study schedule for Week 3 21/09/2025 21/09/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=1/ Week 2 Achievements: Gained fundamental understanding of VPC concepts (Module 02-01). Learned about VPC Security and Multi-VPC features (Module 02-02). Studied VPN, DirectConnect, LoadBalancer, and ExtraResources for hybrid connectivity (Module 02-03). Attended Vietnam Cloud Day 2025 and started hands-on lab to create a VPC. Successfully completed EC2 instance launch lab. Practiced and repeated lab exercises for better hands-on experience. Completed weekly journal and set learning reflection for Week 2. "
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Goals: Master the basic knowledge of Amazon EC2 services: instance types, AMI, Backup, Key Pair, EBS, Instance Store, User Data, Meta Data. Understand the operating mechanism and benefits of EC2 Auto Scaling, and have an overview of related services such as EFS, FSx, Lightsail, MGN. Tasks to be carried out this week: Day Task Start Date Completion Date References 2 - Learn about EC2 instance types and their functions - Module 03-01-01 - Amazon Elastic Compute Cloud (EC2) - Instance type 22/09/2025 22/09/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=1/ 3 - Understand AMI / Backup / Key Pair concepts and Elastic Block Store (EBS) - Module 03-01-02 - Amazon EC2 - AMI / Backup / Key Pair - Module 03-01-03 - Amazon EC2 - Elastic block store 23/09/2025 23/09/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=1/ 4 - Learn about important EC2 features such as Instance Store, User Data, Metadata - Module 03-01-04 - Amazon EC2 - Instance store - Module 03-01-05 - Amazon EC2 - User data - Module 03-01-06 - Amazon EC2 - Meta data 24/09/2025 24/09/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=1/ 5 - Study EC2 Auto Scaling and understand its structure - Module 03-01-07 - Amazon EC2 - EC2 auto scaling - Module 03-02 - EC2 Auto Scaling - EFS/FSx - Lightsail - MGN 25/09/2025 25/09/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=1/ 6 - Practice related lab exercises 26/09/2025 26/09/2025 https://cloudjourney.awsstudygroup.com/ 7 - Practice related lab exercises 27/09/2025 27/09/2025 https://cloudjourney.awsstudygroup.com/ Sun - Write the worklog for Week 3 - Prepare a study plan for Week 4 28/09/2025 28/09/2025 https://cloudjourney.awsstudygroup.com/ Achievements in Week 3: Gained a clear understanding of different EC2 instance types and their appropriate use cases. Mastered the concepts of AMI, Backup, Key Pair, and Elastic Block Store (EBS), including how to create and manage them. Became familiar with extended EC2 features: Instance Store, User Data, Metadata, and their practical applications. Understood the mechanism and benefits of EC2 Auto Scaling, along with an overview of related services such as EFS, FSx, Lightsail, and MGN. Completed several lab practices, reinforcing learned knowledge and improving hands-on skills with AWS Console \u0026amp; CLI. Successfully connected and interacted with members of the First Cloud Journey, building a foundation for group learning and collaboration. Wrote the worklog for Week 3 and created a study plan for Week 4. "
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Goals: Master knowledge about AWS storage services: types of services, data management, security, and performance optimization. Understand the operation of S3, Glacier, Snow Family, Storage Gateway, and Backup, and how to apply them in practice. Complete labs to reinforce knowledge and develop hands-on skills on AWS Console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date End Date Reference Materials Mon - Study Module 04-01 - AWS Storage Services 29/09/2025 29/09/2025 https://cloudjourney.awsstudygroup.com/ Tue - Study Module 04-02 - Amazon S3: Access Point, Storage Class 30/09/2025 30/09/2025 https://cloudjourney.awsstudygroup.com/ Wed - Study Module 04-03 - S3 Static Website \u0026amp; CORS, Control Access, Object Key \u0026amp; Performance, Glacier 01/10/2025 01/10/2025 https://cloudjourney.awsstudygroup.com/ Thu - Study Module 04-04 - Snow Family, Storage Gateway, Backup 02/10/2025 02/10/2025 https://cloudjourney.awsstudygroup.com/ Fri - Complete labs related to Module 04-01 ~ 04-04 03/10/2025 03/10/2025 https://cloudjourney.awsstudygroup.com/ Sat - Continue labs - Translate 1 related blog article 04/10/2025 04/10/2025 https://cloudjourney.awsstudygroup.com/ Sun - Translate 2 remaining blog articles - Write weekly worklog - Plan study tasks for Week 5 05/10/2025 05/10/2025 https://cloudjourney.awsstudygroup.com/ Week 4 Outcomes: Gained a clear understanding of AWS storage services, their functions, and practical applications. Mastered knowledge of Amazon S3: Access Point, Storage Class, Static Website, CORS, Object Key, Performance, Glacier. Became familiar with Snow Family, Storage Gateway, and Backup, and learned how to deploy and manage them. Completed hands-on labs, reinforcing skills on AWS Console \u0026amp; CLI. Successfully translated blog articles, improving reading comprehension and knowledge synthesis. Wrote the weekly worklog and planned study tasks for Week 5. "
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Gain a solid understanding of AWS security and identity management services. Understand the Shared Responsibility Model and the division of responsibilities between AWS and customers. Learn and apply the concepts of IAM, Cognito, AWS Organization, Identity Center, KMS, and Security Hub for secure resource management. Practice hands-on labs and conduct additional research to strengthen AWS security skills. Complete the team project proposal by the end of the week. Tasks to be carried out this week: Day Task Start Date End Date Reference Source Mon - Study Module 05-01: Shared Responsibility Model 06/10/2025 06/10/2025 https://cloudjourney.awsstudygroup.com/ Tue - Study Module 05-02: Amazon Identity and Access Management (IAM) 07/10/2025 07/10/2025 https://cloudjourney.awsstudygroup.com/ Wed - Study Module 05-03: Amazon Cognito - Study Module 05-04: AWS Organization 08/10/2025 08/10/2025 https://cloudjourney.awsstudygroup.com/ Thu - Study Module 05-05: AWS Identity Center - Study Module 05-06: Amazon Key Management Service (KMS) 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ Fri - Study Module 05-07: AWS Security Hub - Study Module 05-08: Hands-on and Additional Research 10/10/2025 10/10/2025 https://cloudjourney.awsstudygroup.com/ Sat - Practice the related labs of week 5 11/10/2025 11/10/2025 https://cloudjourney.awsstudygroup.com/ Sun - Complete the Team Project Proposal - Write the Week 5 Worklog - Prepare the Week 6 Study Plan 12/10/2025 12/10/2025 https://cloudjourney.awsstudygroup.com/ Week 5 Achievements: Clearly understood the Shared Responsibility Model and the security responsibilities of both AWS and users. Mastered AWS IAM concepts: managing users, groups, roles, policies, MFA, and best practices. Gained experience with Amazon Cognito and AWS Identity Center for centralized user authentication and identity management. Learned how to organize and manage multiple accounts using AWS Organizations. Understood AWS KMS operations: key management, rotation, and encryption integration with other AWS services. Became familiar with AWS Security Hub to monitor and assess security posture. Completed hands-on labs and additional research to strengthen practical security skills. Finished the team project proposal, outlining ideas, scope, and implementation plan. "
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Understand the architecture and operating principles of Microservice Architecture. Master how services communicate through gRPC and Apache Kafka. Review and consolidate knowledge to prepare for the midterm exam. Design a system architecture diagram for the group project, showing the communication flow between services. Tasks to be implemented this week: Day Task Start Date End Date Reference Mon - Review Monolithic architecture and compare it with Microservice\n- Learn basic principles of Microservice design 13/10/2025 13/10/2025 https://cloudjourney.awsstudygroup.com/ Tue - Learn gRPC: concept, operating mechanism, Protocol Buffers 14/10/2025 14/10/2025 https://cloudjourney.awsstudygroup.com/ Wed - Learn Apache Kafka 15/10/2025 15/10/2025 https://cloudjourney.awsstudygroup.com/ Thu - Learn how to integrate gRPC and Kafka in a microservice system 16/10/2025 16/10/2025 https://cloudjourney.awsstudygroup.com/ Fri - Review and summarize knowledge from week 1–6 17/10/2025 17/10/2025 https://cloudjourney.awsstudygroup.com/ Sat - Design system architecture diagram for the group project 18/10/2025 18/10/2025 https://cloudjourney.awsstudygroup.com/ Sun - Complete the system architecture diagram\n- Write week 6 work log 19/10/2025 19/10/2025 https://cloudjourney.awsstudygroup.com/ Week 6 Achievements: Clearly understand the difference between Monolithic and Microservice Architecture. Learn about gRPC and Kafka. Know how to integrate gRPC and Kafka in a microservice system. Complete the group project system architecture diagram. Be ready for the midterm exam. "
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Learn about AWS Lambda. Learn about the AWS CLI and how to deploy a Java function to Lambda using the AWS CLI. Understand how API Gateway and Lambda work together. 1 Tasks to be implemented this week: Day Task Start Date Completion Date Resources Mon - Get an overview of AWS Lambda: concepts, architecture, advantages of the serverless model, and how Lambda executes code. 20/10/2025 20/10/2025 https://cloudjourney.awsstudygroup.com/ Tue - Study the AWS CLI: its functions, how to install and configure access to an AWS account using an IAM User; learn the syntax of basic commands. 21/10/2025 21/10/2025 https://cloudjourney.awsstudygroup.com/ Wed - Practice installing the AWS CLI on a personal machine, configuring credentials, and testing some resource management commands (S3, EC2, Lambda). 22/10/2025 22/10/2025 https://cloudjourney.awsstudygroup.com/ Thu - Learn the process of deploying a Java Function to AWS Lambda: preparing a Maven project, packaging a .jar file, creating and uploading the function using the AWS CLI. 23/10/2025 23/10/2025 https://cloudjourney.awsstudygroup.com/ Fri - Practice deploying a Java Function to Lambda using the AWS CLI, assigning the appropriate IAM role, and testing the function\u0026rsquo;s operation. 24/10/2025 24/10/2025 https://cloudjourney.awsstudygroup.com/ Sat - Understand how API Gateway connects with AWS Lambda to build a backend API; configure endpoints and test API calls. 25/10/2025 25/10/2025 https://cloudjourney.awsstudygroup.com/ Sun - Consolidate learned knowledge: the relationship between Lambda – API Gateway – AWS CLI, document the deployment process, and prepare the weekly report. - Write the work log for week 6. 26/10/2025 26/10/2025 https://cloudjourney.awsstudygroup.com/ Week 7 Achievements: 1. Clear Understanding of AWS Lambda Grasped the concept and role of AWS Lambda in the serverless model. Understood Lambda\u0026rsquo;s operational mechanism: the trigger process, function execution, and returning results. Distinguished the differences between Lambda and EC2 regarding: Infrastructure Management Usage Costs Scalability and Flexibility. 2. Proficiency in Using the AWS CLI Successfully installed and configured the AWS CLI on a personal machine using an IAM User. Practiced basic commands: Listing S3 buckets. Creating and viewing information for a Lambda function. Managing basic resources on AWS. Understood how the AWS CLI sends API requests and the authentication mechanism using Access Key/Secret Key. 3. Practical Experience Deploying a Java Function to AWS Lambda via AWS CLI Built a Java project using Maven, created a handler class, and packaged it into a .jar file. Used the aws lambda create-function command to upload and deploy the function. Assigned an appropriate IAM Role to grant the Lambda function execution permissions. Tested the function\u0026rsquo;s operation with the command: "
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Complete the group proposal. Review key concepts in preparation for the midterm exam. Strengthen missing or unclear knowledge areas. Tasks Planned for This Week: Day Task Start Date Completion Date Reference Material 2 - Complete the group proposal.\n- Review security design concepts: + IAM, MFA, SCP, Encryption (KMS, TLS/ACM), Security Groups, NACLs, GuardDuty, Shield, WAF, Secrets Manager 27/10/2025 27/10/2025 https://cloudjourney.awsstudygroup.com/ 3 - Review how to design resilient and highly available architectures: + Multi-AZ, Multi-Region, DR Strategies, Auto Scaling, Route 53, Load Balancing, Backup \u0026amp; Restore 28/10/2025 28/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Review high-performance system design concepts: + EC2 Auto Scaling, Lambda, Fargate, S3, EFS, EBS, Caching, CloudFront, Global Accelerator 29/10/2025 29/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Review cost optimization strategies: + Cost Explorer, Budgets, Savings Plans, Lifecycle Policies, NAT Gateway Optimization, Storage Tiering 30/10/2025 30/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Participate in the FCJ midterm assessment. 31/10/2025 31/10/2025 https://cloudjourney.awsstudygroup.com/ Results Achieved in Week 8: Completed the group proposal with clear content, logical structure, and alignment with the project requirements. Reviewed and reinforced knowledge of AWS security design, including IAM, MFA, SCP, Encryption (KMS, TLS/ACM), Security Groups, NACLs, GuardDuty, Shield, WAF, and Secrets Manager. Gained a solid understanding of resilient and highly available architecture design principles, emphasizing the use of Multi-AZ, Multi-Region, DR Strategies, Auto Scaling, Route 53, Load Balancing, and Backup \u0026amp; Restore to enhance system availability and recovery. Strengthened knowledge of high-performance system design, particularly in leveraging EC2 Auto Scaling, Lambda, Fargate, S3, EFS, EBS, Caching, CloudFront, and Global Accelerator to optimize system performance and content delivery. Improved understanding of cost optimization in AWS, including the use of Cost Explorer, Budgets, Savings Plans, Lifecycle Policies, NAT Gateway Optimization, and Storage Tiering. Successfully completed the FCJ midterm assessment, effectively applying the knowledge gained during the week’s review. "
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Learn the fundamentals of AWS IoT Core. Explore AWS Amplify for frontend development and hosting. Understand how these services integrate within the AWS ecosystem. Apply the knowledge to support the group project. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Overview of AWS IoT Core: concepts, architecture, and device communication 11/03/2025 11/03/2025 https://cloudjourney.awsstudygroup.com/ 3 - Study MQTT, Things, Device Shadow, and Rules Engine 11/04/2025 11/04/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about AWS Amplify: features and frontend hosting 11/05/2025 11/05/2025 https://cloudjourney.awsstudygroup.com/ 5 - Explore integration of Amplify with Auth, API, and Storage services 11/06/2025 11/06/2025 https://cloudjourney.awsstudygroup.com/ 6 - Research real-world use cases of AWS IoT Core and Amplify 11/07/2025 11/07/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Understood the architecture and working principles of AWS IoT Core. Learned key components such as Things, Device Shadow, MQTT, and Rules Engine. Gained knowledge of AWS Amplify for frontend hosting and development. Understood how Amplify integrates with authentication, APIs, and storage services. Expanded knowledge of the AWS ecosystem for the group project. "
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Learn about VPC in AWS\nWeek 3: Master the basics of EC2\nWeek 4: Explore AWS storage services and get familiar with the AWS CLI command system\nWeek 5: Learn and practice AWS security content (IAM, Cognito, Organization, Identity Center, KMS, Security Hub)\nWeek 6: Learn Microservice Architecture, learn gRPC and Apache Kafka, review for midterms, and complete a group project system architecture drawing.\nWeek 7: Learn AWS Lambda, install and use AWS CLI, practice deploying Java functions to Lambda and building APIs with API Gateway\nWeek 8: Complete a group proposal, review comprehensive AWS knowledge (security, architecture, performance, costs), and take the midterm exam\nWeek 9: Learn AWS IoT Core and AWS Amplify to expand your knowledge of the AWS ecosystem and serve your project\nWeek 10: Learned Terraform and GitHub Actions while deploying the basic project\u0026rsquo;s infrastructure architecture\nWeek 11: Deploy entire AWS infrastructure using Terraform and build GitHub Actions pipeline for the system\nWeek 12: Complete group project demo, prepare presentation slides and compile personal worklog\n"
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/5-workshop/5.2-prerequiste/",
	"title": "Preparation Steps",
	"tags": [],
	"description": "",
	"content": "IAM Permissions Attach the following IAM permission policy to your AWS user account to deploy and clean up resources in this workshop.\n{\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;WorkshopPermissions\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;ec2:RunInstances\u0026#34;,\r\u0026#34;ec2:TerminateInstances\u0026#34;,\r\u0026#34;ec2:StopInstances\u0026#34;,\r\u0026#34;ec2:StartInstances\u0026#34;,\r\u0026#34;ec2:Describe*\u0026#34;,\r\u0026#34;ec2:CreateTags\u0026#34;,\r\u0026#34;ec2:RebootInstances\u0026#34;,\r\u0026#34;iam:CreateRole\u0026#34;,\r\u0026#34;iam:AttachRolePolicy\u0026#34;,\r\u0026#34;iam:PassRole\u0026#34;,\r\u0026#34;iam:PutRolePolicy\u0026#34;,\r\u0026#34;iam:GetRole\u0026#34;,\r\u0026#34;iam:ListRoles\u0026#34;,\r\u0026#34;iam:CreatePolicy\u0026#34;,\r\u0026#34;cloudwatch:PutMetricData\u0026#34;,\r\u0026#34;cloudwatch:GetMetricStatistics\u0026#34;,\r\u0026#34;cloudwatch:ListMetrics\u0026#34;,\r\u0026#34;cloudwatch:PutMetricAlarm\u0026#34;,\r\u0026#34;cloudwatch:DeleteAlarms\u0026#34;,\r\u0026#34;cloudwatch:DescribeAlarms\u0026#34;,\r\u0026#34;events:PutRule\u0026#34;,\r\u0026#34;events:PutTargets\u0026#34;,\r\u0026#34;events:DeleteRule\u0026#34;,\r\u0026#34;events:RemoveTargets\u0026#34;,\r\u0026#34;events:DescribeRule\u0026#34;,\r\u0026#34;events:ListRules\u0026#34;,\r\u0026#34;ssm:GetDocument\u0026#34;,\r\u0026#34;ssm:ListDocuments\u0026#34;,\r\u0026#34;ssm:DescribeInstanceInformation\u0026#34;,\r\u0026#34;ssm:GetAutomationExecution\u0026#34;,\r\u0026#34;ssm:StartAutomationExecution\u0026#34;,\r\u0026#34;sns:CreateTopic\u0026#34;,\r\u0026#34;sns:Subscribe\u0026#34;,\r\u0026#34;sns:Publish\u0026#34;,\r\u0026#34;sns:DeleteTopic\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Lab Environment We will build the infrastructure from scratch to understand how to install the Agent and configure permissions (IAM Role).\nBasic Requirements:\nDefault VPC: Ensure that your us-east-1 region has a Default VPC available. If you have already deleted it, create a new VPC with a simple Public Subnet configuration.\nInternet Access: The EC2 Instance in this lab will need Internet connectivity to download the stress package and amazon-cloudwatch-agent.\nCheck Default VPC in Console\nAfter ensuring the permissions and VPC are in place, you are ready to proceed to the implementation phase.\n"
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "IoT-Based Alcohol Violation Detection System A Secure AWS Serverless Solution with Biometric Authentication and DevSecOps 1. Executive Summary This project develops a comprehensive IoT–Cloud platform to monitor and record alcohol concentration violations.\nThe system is composed of two major components:\nDedicated Edge Device: Used by authorized officers, requiring fingerprint (hybrid) authentication before operation. The fingerprint SlotID is verified against a data pool (DynamoDB) on the cloud to unlock the device. Public Web Portal: A web interface that allows anyone to view dashboards and search for their violation history using National ID (CCCD). The backend is fully serverless. The frontend CI/CD workflow is automated using AWS Amplify, while backend infrastructure is managed using Terraform (IaC) and deployed through AWS CodePipeline, following DevSecOps best practices.\n2. Context and Solution The Problem The current manual inspection process contains several critical weaknesses in terms of authentication, transparency, and data management.\nLack of Accountability: The system does not have any mechanism to verify the identity of the operating personnel (who conducted the inspection?). This leads to potential misuse of equipment, procedural errors, and a lack of responsibility when incidents or complaints occur. Lack of Transparency: Citizens who have undergone an inspection have no official, fast, and reliable channel to verify or review their violation records. Fragmented and Unsearchable Data: Violation data (if recorded) is often stored in scattered logbooks or separate Excel files, without centralized management. This makes it nearly impossible—or extremely time-consuming—to look up a person’s violation history (e.g., by national ID number). As a result, it wastes resources and undermines public trust. Proposed Solution This system provides a secure, transparent, and automated enforcement platform by:\nOperator Authentication: Using fingerprint sensors integrated with AWS IoT Core to ensure only authorized personnel can unlock the device. Auditing: Every violation is automatically tagged with the operator’s ID for accountability. Centralized Data Management: Violation data is transmitted via IoT Core, processed by AWS Lambda, and stored in DynamoDB. Public Lookup Portal: A public website built with Amplify + CloudFront, with all API endpoints protected by AWS WAF. Automated CI/CD Pipelines: Frontend CI/CD using Amplify (connected to GitHub). Backend CI/CD using CodePipeline and Terraform. Key Benefits:\nImproved transparency and reduced human error. Fast deployment with low operational cost (~14–16 USD/month). Fully compliant with DevSecOps standards. 3. Solution Architecture AWS Services Used Category Service Purpose IoT \u0026amp; Ingestion AWS IoT Core Receive and route data from IoT devices Compute AWS Lambda 4 functions: Authorize, ProcessViolation, GetDashboard, SearchByCCCD Database Amazon DynamoDB 2 tables: DeviceOfficerMap_Pool, ViolationsDB (2 GSIs) API Amazon API Gateway Provide public REST endpoints Security AWS WAF Protect API and web endpoints Frontend Hosting S3 + CloudFront + Route 53 Host and distribute the website Frontend Framework AWS Amplify Automate frontend CI/CD Backend DevOps CodePipeline + CodeBuild + Terraform Automate IaC deployment Monitoring CloudWatch System monitoring and alerting Hardware Components (Edge) ESP32, MQ-3 (Alcohol Sensor) MAX30102 (Heart Rate / Oxygen Sensor) AS608 (Fingerprint Sensor) 4x4 Keypad, LCD 1602 Display Main Processing Flow Authentication (Hybrid): Officer scans fingerprint → SlotID sent via auth/request → IoT Core → Lambda AuthorizeFunction → Query DynamoDB → Return unlock/deny. Violation Recording: Once unlocked → Measure alcohol level → If exceeded → Input CCCD → Send violations/new → IoT Core → Lambda ProcessViolationFunction → Save to DynamoDB. Public Web Interface: SPA (React/Vue) using Amplify libraries communicates with API Gateway. 4. Technical Implementation Plan Phase Duration Key Tasks Design \u0026amp; IaC Week 1–2 Finalize architecture, write Terraform scripts Firmware (ESP32) Week 3–4 Implement authentication and violation reporting Backend Week 5–7 Develop Lambda functions, IoT Core, and API Gateway Frontend Week 8–9 Build the web UI with React + Amplify CI/CD \u0026amp; Security Week 10–12 Configure pipelines, WAF, and CloudWatch monitoring 5. Timeline \u0026amp; Milestones Month Phase Milestone First month Design + Firmware ESP32 authenticates officers and sends sample violations Second month Backend + API Functional API responding via browser Third month Frontend + CI/CD Public website live with automated deployment 6. Estimated Monthly Budget AWS service: Service Cost (USD) Notes AWS WAF 6.00 1 Web ACL + Rule Route 53 0.50 1 Hosted Zone CodePipeline 1.00 Backend pipeline Amplify (Build + Hosting) 2.72 100 build minutes + 20GB transfer CloudWatch 2.80 5GB logs + 3 alarms IoT Core 1.00 10,000 messages CodeBuild 0.00 Free tier Lambda 0.22 25,000 invocations API Gateway 0.07 20,000 requests DynamoDB 0.02 On-demand capacity Total: ~14.33 USD/month (~14–16 USD actual cost)\nHardware: $25 one-time (ESP32 and sensors) 7. Risk Assessment Risk Impact Level Likelihood Mitigation PII Exposure (CCCD) Critical High Add 2FA or mask last 4 digits of CCCD Network Disconnection High Medium ESP32 temporarily stores logs and syncs when online Sensor Malfunction High Medium Calibrate periodically, apply signal filtering CI/CD Failure Medium Medium Use staging pipeline and code review for IaC 8. Expected Outcomes Technical: Full-stack IoT–Cloud system with biometric authentication, serverless backend, and DevSecOps automation. Practical: Improves transparency and accountability of enforcement officers. Academic: A graduation project demonstrating mastery of AWS Cloud, IoT, and modern DevOps. "
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Learn Terraform for Infrastructure as Code (IaC). Learn GitHub Actions for CI/CD automation. Apply Terraform and GitHub Actions to deploy project infrastructure. Build a basic DevOps workflow for the group project. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn Terraform fundamentals: providers, resources, state, and variables 11/10/2025 11/10/2025 https://cloudjourney.awsstudygroup.com/ 3 - Practice writing Terraform configurations and managing infrastructure 11/11/2025 11/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn GitHub Actions basics: workflows, jobs, runners, and YAML syntax 11/12/2025 11/12/2025 https://cloudjourney.awsstudygroup.com/ 5 - Create CI/CD pipelines using GitHub Actions for deployment 11/13/2025 11/13/2025 https://cloudjourney.awsstudygroup.com/ 6 - Deploy project infrastructure using Terraform 11/14/2025 11/14/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Learned how to use Terraform to define and manage infrastructure as code. Understood Terraform concepts: provider, resource, state, and variable. Built basic GitHub Actions workflows for CI/CD automation. Integrated Terraform with GitHub Actions for automated deployment. Successfully deployed the initial infrastructure architecture for the project. "
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Build AWS infrastructure using Terraform. Set up GitHub Actions for automated deployment. Complete the CI/CD pipeline for the backend. Ensure security and organize infrastructure according to the project architecture Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Analyze system architecture and identify AWS resources to deploy 11/17/2025 11/17/2025 https://cloudjourney.awsstudygroup.com/ 3 - Write Terraform scripts for core services: S3, DynamoDB, IAM 11/18/2025 11/18/2025 https://cloudjourney.awsstudygroup.com/ 4 - Deploy IoT Core, Lambda, and API Gateway using Terraform 11/19/2025 11/19/2025 https://cloudjourney.awsstudygroup.com/ 5 - Configure Terraform backend 11/20/2025 11/20/2025 https://cloudjourney.awsstudygroup.com/ 6 - Set up GitHub Actions for Terraform plan / apply 11/21/2025 11/21/2025 https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Complete AWS infrastructure using Terraform. Successfully deployed important services: AWS IoT Core Lambda API Gateway DynamoDB IAM Configure Terraform remote state using S3 and lock using DynamoDB. Build automated GitHub Actions pipeline: Run terraform init terraform plan terraform apply Pipeline works stably, can be redeployed many times without errors. Infrastructure closely follows the project architecture diagram. "
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Support the team in testing and integrating the entire system. Complete the team project demo. Prepare the presentation slides. Summarize and complete all personal tasks. Finish the worklog and final report. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Collaborate on backend testing with the team and verify all infrastructure deployed via Terraform 11/24/2025 11/24/2025 https://cloudjourney.awsstudygroup.com/ 3 - Assist the team in testing the system demo 11/25/2025 11/25/2025 https://cloudjourney.awsstudygroup.com/ 4 - Prepare slide content (infrastructure, CI/CD, architecture) 11/26/2025 11/26/2025 https://cloudjourney.awsstudygroup.com/ 5 - Run a full demo test 11/27/2025 11/27/2025 https://cloudjourney.awsstudygroup.com/ 6 - Write Week 12 worklog and summarize personal tasks 11/28/2025 11/28/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: The system is deployed stably for demo. Successfully demoed the group project according to the designed architecture. Full presentation slides: Infrastructure architecture How to deploy with Terraform CI/CD with GitHub Actions Completed all individual tasks: Terraform configuration CI/CD pipeline Deployment documentation Worklog and report are fully submitted. "
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/5-workshop/5.3-iam/",
	"title": "Create IAM Role",
	"tags": [],
	"description": "",
	"content": "Create IAM Role First, we need to grant permissions to the EC2 Instance so it can send data to CloudWatch and allow Systems Manager to control it (for reboot).\nSteps Access IAM Console Open AWS Management Console and select IAM service. Create a new Role Select Roles from the left menu. Click Create role. Configure Trusted Entity Type Under Trusted entity type, select AWS service. Under Service or use case, select EC2. Click Next. Attach Permissions Policies In the Permissions policies search box, find and select (check the box) 2 permissions: CloudWatchAgentServerPolicy (To push logs/metrics) AmazonSSMManagedInstanceCore (To connect Session Manager and execute Automation) Click Next. Name the Role Set the Role name as: EC2-Monitor-AutoFix-Role Click Create role. You have successfully created an IAM Role to grant permissions to the EC2 instance.\n"
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Enhance data visibility with Cribl Search and Amazon Managed Grafana This blog demonstrates how Cribl Search can be combined with Amazon Managed Grafana to increase data visibility. Cribl Search enables direct querying of logs and data without indexes, while Grafana enables visualization into real-time dashboards. This solution helps enterprises monitor cloud infrastructure, manage application performance, and enhance security while optimizing operational costs.\nBlog 2 - Enabling customers to deliver production-ready AI agents at scale The blog post discusses how AWS helps customers deploy production-ready AI agents at scale, ensuring security, reliability, and ease of adoption across multiple industries.\nBlog 3 - Goal-Oriented Growth: Driving AWS Marketplace Success with COSS This blog highlights how AWS Marketplace leverages Commercial Open Source Software (COSS) to drive goal-oriented growth for both customers and sellers. It explains the value of COSS in delivering innovation, flexibility, and faster adoption, while also providing guidance for organizations to align business goals with technology adoption through AWS Marketplace.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders\nDate \u0026amp; Time: 09:00, September 18 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent Overview Vietnam Cloud Day is an annual AWS event designed to connect engineers, developers, and enterprises in Vietnam.\nThe Ho Chi Minh City Connect Edition for Builders focused on cloud-native architecture, security, data management, and the integration of AI technologies to empower developers in building scalable, secure, and innovative applications.\nMain Activities Opening keynote by AWS Vietnam representatives introducing the “Cloud for Builders” vision. Multiple technical sessions focusing on: Cost optimization and performance improvement on AWS. Implementing serverless and container-based architectures. Introduction to new AWS services such as Bedrock, Amazon Q, and Amazon Aurora. AWS Builder Lab area where attendees could practice hands-on exercises with guidance from AWS experts. Networking sessions with AWS professionals and other participants. Outcomes and Value Gained Gained a deeper understanding of cloud-native and microservices architecture. Learned how to integrate AI and GenAI into cloud applications for optimization and automation. Improved communication and technical discussion skills through Q\u0026amp;A sessions with AWS specialists. Received technical materials and learning credits from AWS, enhancing my self-study opportunities. Event 2 Event Name: Data Science on AWS Workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent Overview The GenAI-powered App-DB Modernization workshop focused on modernizing legacy applications and databases using advanced methods and tools. It covered Domain-Driven Design (DDD), event-driven architecture, microservices patterns, and AI-powered development tools to help organizations transition to cloud-native, scalable systems.\nMain Activities Technical presentations on modern application architecture and the drawbacks of legacy systems. Deep dive into Domain-Driven Design (DDD) with practical case studies and the event storming technique. Exploration of event-driven architecture patterns: Publish/Subscribe, Point-to-point, and Streaming. Comparison of compute evolution: EC2 → ECS → Fargate → Lambda with criteria for choosing serverless solutions. Hands-on demonstration of Amazon Q Developer, an AI tool for SDLC automation and code transformation. Interactive discussions on application modernization strategies and the 7Rs framework. Networking opportunities with AWS experts and practitioners. Outcomes and Value Gained Gained a comprehensive understanding of DDD and event-driven architecture for building scalable, loosely-coupled systems. Learned practical techniques like event storming to model business processes into technical solutions. Understood the phased approach to modernization with focus on ROI measurement and business agility. Discovered how Amazon Q Developer can automate code transformation and boost productivity. Developed insights into integrating AI tools into the development workflow for faster delivery. Strengthened the connection between business requirements and technical architecture through ubiquitous language. "
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/5-workshop/5.4-ec2/",
	"title": "Launch EC2 Instance",
	"tags": [],
	"description": "",
	"content": "Launch EC2 Instance We will create a server that simulates a web application and pre-install necessary tools.\nSteps Access EC2 Console Open AWS Management Console and select EC2 service. Select Launch instances. Configure Instance Details Name: Set as Web-Server-Test OS Images: Select Amazon Linux 2023 AMI Instance type: Select t2.micro or t3.micro Key pair: Select \u0026ldquo;Proceed without a key pair\u0026rdquo; (We will use Session Manager, no need for SSH Key) Configure Network Settings\nLeave as default (ensure it\u0026rsquo;s in Public Subnet with Public IP) Configure Advanced details\nIAM instance profile: Select EC2-Monitor-AutoFix-Role created in step 1 Configure User Data Expand the Advanced details section Copy and paste the following script into the User Data field to automatically install Agent and testing tools: #!/bin/bash dnf update -y dnf install amazon-cloudwatch-agent stress -y Launch Instance Click Launch instance Wait a few minutes for the Instance to transition to Running state "
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/5-workshop/5.5-cloudwatch-agent/",
	"title": "Configure CloudWatch Agent",
	"tags": [],
	"description": "",
	"content": "Configure CloudWatch Agent By default, EC2 does not monitor RAM (Memory) metrics. We need to configure the Agent to collect this metric.\nSteps Access EC2 Instance via Session Manager In the EC2 instance list, select Web-Server-Test Click Connect Select the Session Manager tab Click Connect to open the command-line interface Create Agent Configuration File Create the configuration file using the following command (Copy the entire block and paste into terminal): sudo tee /opt/aws/amazon-cloudwatch-agent/bin/config.json \u0026lt;\u0026lt;EOF { \u0026#34;agent\u0026#34;: { \u0026#34;metrics_collection_interval\u0026#34;: 60, \u0026#34;run_as_user\u0026#34;: \u0026#34;root\u0026#34; }, \u0026#34;metrics\u0026#34;: { \u0026#34;metrics_collected\u0026#34;: { \u0026#34;mem\u0026#34;: { \u0026#34;measurement\u0026#34;: [ \u0026#34;mem_used_percent\u0026#34; ], \u0026#34;metrics_collection_interval\u0026#34;: 60 } } } } EOF Start Agent to Load Configuration Run the following command: sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c file:/opt/aws/amazon-cloudwatch-agent/bin/config.json You have successfully configured CloudWatch Agent to monitor the memory (RAM) metrics of your EC2 instance.\n"
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "CloudWatch Monitoring and Auto-Recovery System Overview In this lab, you will build a monitoring and auto-recovery system using AWS CloudWatch, EventBridge, and Systems Manager. This system will continuously monitor the RAM usage of an EC2 instance and automatically restart the server when memory exceeds the 80% threshold.\nBenefits of This System Continuous Monitoring: CloudWatch Agent collects memory metrics every 60 seconds Real-time Alerts: SNS Topic sends email notifications when an Alarm is triggered Automatic Recovery: EventBridge Rule triggers Systems Manager to automatically restart the instance No Manual Intervention: The system operates 24/7 without user intervention Content Workshop overview Prerequisite Create IAM Role Launch EC2 Instance Configure CloudWatch Agent Create CloudWatch Alarm Setup EventBridge Rule Testing (Chaos Testing) Cleanup Resources "
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/5-workshop/5.6-cloudwatch-alarm/",
	"title": "Create CloudWatch Alarm",
	"tags": [],
	"description": "",
	"content": "Create CloudWatch Alarm We will create an alarm when RAM usage exceeds 80%.\nSteps Access CloudWatch Console\nOpen AWS Management Console and select CloudWatch service. Create a new Alarm\nSelect All alarms -\u0026gt; Create alarm Select Metric Click Select metric Select CWAgent -\u0026gt; ImageId, InstanceId, InstanceType Find the mem_used_percent metric for the Web-Server-Test instance Configure Conditions Statistic: Average Period: 1 minute Threshold type: Static Threshold value: 80 (Greater than 80%) Configure Actions Notification: Select Create new topic Topic name: Set as Admin-Alerts Email: Enter your email Click Create topic Important note: Do not select \u0026ldquo;EC2 Action\u0026rdquo; here (because RAM metric is a custom metric and does not support direct reboot). We will use EventBridge instead. Remember to check your email to Confirm Subscription. Name the Alarm Alarm name: High-Memory-Auto-Reboot Click Create alarm You have successfully created a CloudWatch Alarm to monitor the RAM usage of your EC2 instance.\n"
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/5-workshop/5.7-eventbridge-rule/",
	"title": "Setup EventBridge Rule",
	"tags": [],
	"description": "",
	"content": "Setup EventBridge Rule This step connects the Alarm with the Reboot action through Systems Manager.\nSteps Access Amazon EventBridge Console Open AWS Management Console and select EventBridge service. Create a new Rule Select Rule with an event pattern Select Rules -\u0026gt; Create rule Configure Events (Step 4a: Triggering Events) In the Event section on the left, select AWS SERVICE EVENT Search event: CloudWatch Alarm State Change Drag and drop into the Triggering Event box Configure Targets (Step 4b: Targets) On the left section, select the Targets section Search target: EC2 Reboot Instance Drag and drop into the Targets box In the Target configuration section, assign the EC2 instance ID that you created in the previous steps Select Create a new role for this specific resource Finish Click Create rule to complete Name: Rule-Reboot-On-High-Memory You have successfully set up an EventBridge Rule to automatically restart the EC2 instance when RAM exceeds 50%.\n"
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/5-workshop/5.8-test/",
	"title": "Testing (Chaos Testing)",
	"tags": [],
	"description": "",
	"content": "Testing (Chaos Testing) Simulate a memory overflow issue to test the system.\nSteps Return to the EC2 Session Manager interface\nAccess EC2 Console Select the Web-Server-Test instance Click Connect Select the Session Manager tab -\u0026gt; Connect Run stress command to push RAM to alarm level\nRun the following command (approximately 850MB for t2.micro): stress --vm 1 --vm-bytes 850M --vm-hang 300 Observe the Results On CloudWatch Alarm:\nStatus changes to red (ALARM) In Email:\nReceive alert from SNS On Session Manager:\nConnection is disconnected (server is restarting) On EC2 Console:\nInstance status changes to Stopping then Running After Instance restarts:\nAlarm will automatically return to OK status Conclusion You have successfully built an automated system for issue recovery! When memory exceeds 80%, the system will automatically:\nSend alert via email Trigger EventBridge Rule Call Systems Manager to restart the instance Instance automatically restarts and returns to OK status "
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/5-workshop/5.9-clean-material/",
	"title": "Cleanup Resources",
	"tags": [],
	"description": "",
	"content": "Cleanup Resources After completing the above steps, you should clean up AWS resources to avoid unnecessary costs.\nSteps Delete EventBridge Rule\nAccess Amazon EventBridge Console Select Rules Find and select Rule-Reboot-On-High-Memory Click Delete Delete CloudWatch Alarm\nAccess CloudWatch Console Select All alarms Find and select High-Memory-Auto-Reboot Click Delete alarm Delete SNS Topic\nAccess SNS Console Select Topics Find and select Admin-Alerts Click Delete Delete EC2 Instance\nAccess EC2 Console Select Instances Find and select Web-Server-Test Click Instance State -\u0026gt; Terminate Confirm Terminate Delete IAM Role\nAccess IAM Console Select Roles Find and select EC2-Monitor-AutoFix-Role Click Delete Confirm Delete role Important Notes Make sure you no longer need these resources before deleting Deletion is permanent and cannot be recovered After deletion, you will not be charged for these resources "
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://bacdang2k4.github.io/fcj-worklog-web/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]